{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing a Univariate Function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locating extrema in a function (analytical solution)\n",
    "\n",
    "Let's start with a basic idea: we are trying to find extrema (either a minimum or a maximum value) returned by a univariate scalar function. We already learned how to do this analytically in our calculus courses, right?  We just find where the derivative of our function is equal to zero!\n",
    "\n",
    "It's always nice to start out with a problem that we can solve analytically, so let's look at a cubic polynomial. \n",
    "\n",
    "$$y(x) = -1.6x^3 + 5x^2 + 8x - 23$$\n",
    "\n",
    "We know that we can expect, at most, 3 real roots for this polynomial, and we can expect at most 2 extrema (either maxima or minima). In the cell below, graph this function on the domain $x = [-2.3, 4]$; plot the x-axis to visualize the root locations, and find any roots in that domain using an algorithm of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lambda x: -1.6*x**3 + 5*x**2 + 8*x - 23\n",
    "xset = np.linspace(-2.3, 4, 100)\n",
    "\n",
    "## Graphing function\n",
    "plt.figure(1, figsize = (5,5))\n",
    "plt.plot(xset, y(xset))\n",
    "plt.hlines(0, -2.3, 4, linestyle = 'dashed', color = 'black', linewidth = 0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xguess = [-2, 2, 3.5]\n",
    "roots  = [opt.newton(y, x0) for x0 in xguess]\n",
    "print(f'The roots are located at x = {np.round(roots, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a graphical analysis is usually informative, we can be more precise in our solution by finding the roots of the derivative of y(x), i.e., by solving:\n",
    "\n",
    "$$y^{\\prime}(x) = -4.8x^2 + 10x + 8 = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yp = lambda x: -4.8*x**2 + 10*x +8\n",
    "ypp = lambda x: -9.6*x + 10\n",
    "\n",
    "plt.figure(1, figsize = (5,5))\n",
    "plt.plot(xset, yp(xset), label = 'first derivative of y')\n",
    "plt.plot(xset, ypp(xset), label = 'second derivative of y')\n",
    "plt.hlines(0, -2.3, 4, linestyle = 'dashed', color = 'black', linewidth = 0.75)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xguess = [-1, 3]\n",
    "opts   = [opt.newton(yp, x0) for x0 in xguess]\n",
    "print(f'Extreme in this function are located at x = {np.round(opts, 3)}')\n",
    "print(f'The extrema in the function have the values y = {np.round(y(np.array(opts)), 2)}')\n",
    "print(f'The second derivative of the function at these locations are ypp = {np.round(ypp(np.array(opts)), 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y   = lambda x: -1.6*x**3 + 5*x**2 + 8*x - 23\n",
    "dy  = lambda x: -3*1.6*x**2 + 10*x + 8\n",
    "ddy = lambda x: -2*3*1.6*x + 10\n",
    "\n",
    "x = 100.0\n",
    "while abs(dy(x)) > 1e-8:\n",
    "    print(round(x,4), round(y(x),4), round(dy(x),4))\n",
    "    x = x - dy(x)/ddy(x)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y   = lambda x: -1.6*x**3 + 5*x**2 + 8*x - 23\n",
    "# dy  = lambda x: -3*1.6*x**2 + 10*x + 8\n",
    "# ddy = lambda x: -2*3*1.6*x + 10\n",
    "\n",
    "# xrange = [-3, 4]\n",
    "# yrange = [0, 0]\n",
    "# xplot  = np.linspace(xrange[0], xrange[1], 10000)\n",
    "# yplot  = y(xplot)\n",
    "\n",
    "# x = -3\n",
    "# xlist = []\n",
    "# ylist = []\n",
    "# while abs(dy(x)) > 1e-8:\n",
    "#     xlist.append(x)\n",
    "#     ylist.append(y(x))\n",
    "#     plt.plot(xplot,yplot,color = 'black', linewidth = 1)\n",
    "#     plt.plot(xrange,yrange, linestyle = 'dashed', linewidth = 1)\n",
    "#     plt.scatter(xlist, ylist, color = 'red', marker = 'o')\n",
    "#     plt.show()\n",
    "#     #print(round(x,4), round(y(x),4), round(dy(x),4))\n",
    "#     #time.sleep(2) #Add a 2 second pause before going on to next iteration\n",
    "#     x = x - dy(x)/ddy(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Routines in Scipy\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/optimize.html\n",
    "\n",
    "<div class = \"alert alert-block alert-info\">\n",
    "    <b>Important</b>: Our very simple Newton method above found extrema without distinguishing between maxima and minima.  It was only looking for a place where the derivative = 0. Scipy optimization algorithms will always ***minimize*** your objective function by default.  If you want to ***maximize*** a function, you can always just multiply the value returned by your objective function by -1.\n",
    "    </div>\n",
    "\n",
    "### `opt.minimize_scalar()`\n",
    "\n",
    "The usage for scipy optimization algorithms is very similar to what we learned with root finding for scalar functions.  Let's start with the most basic option:  minimizing a univariate, scalar function, i.e., the case we are currently considering:\n",
    "\n",
    "$$y(x) = -1.6x^3 + 5x^2 + 8x - 23$$\n",
    "\n",
    "Since y(x) is univariate, we are really just looking for the value of x where y is at a minimum. If all you need to do is find a local minimum in a univariate function, you can use `opt.minimize_scalar()` from `scipy`. It has a very small set of minimum inputs: you only need to provide it with the function, and it will return a local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.minimize_scalar(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = opt.minimize_scalar(y)\n",
    "print(sol)\n",
    "print(sol.fun, sol.nfev, sol.nit, sol.success, sol.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing `opt.minimize()`\n",
    "\n",
    "Digging a bit deeper into packages available in **Scipy**, the `opt.minimize()` function is a more general routine than `opt.minimize_scalar()`. It will allow you to minimize either ***univariate*** or ***multivariate*** scalar functions, the latter meaning that a function accepts multiple arguments (inputs), but it only returns a single, scalar output. \n",
    "\n",
    "### The basic syntax of `opt.minimize()`\n",
    "\n",
    "At a minimum (no pun intended), `opt.minimize()` in its default state requires us to provide the function name and an initial guess at the variable values where we expect to find a minimum:\n",
    "\n",
    "```python\n",
    "opt.minimize(function_name, initial_guess_for_optimum_location)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.minimize(y, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing algorithms with `opt.minimize()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.minimize(y, -1, method = 'Nelder-Mead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.minimize(y, -1, method = 'dogleg', jac = dy, hess = ddy, options = {'disp' : True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.minimize(y, -1, bounds = [(0,1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bit of practice with `opt.minimize()`\n",
    "\n",
    "Before we move on to a more complex problem with constraints, try a few things with our original cubic equation y(x):\n",
    "\n",
    "1. Minimize y(x) using the Powell algorithm.\n",
    "2. Minimize y(x) using the trust-krylov algorithm\n",
    "3. Minimize y(x) using the SLSQP algorithm but place a lower bound of 1 on x.\n",
    "4. Minimize y(x) using the BFGS method; set the tolerance to 1e-8; set the max iterations to 1000, and turn the display on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.minimize(y, 1, method = 'Powell')\n",
    "# opt.minimize(y, 1, jac = dy, hess = ddy, method = 'trust-krylov')\n",
    "# opt.minimize(y, 1.0, method = 'SLSQP', bounds = [(1.0, None)])\n",
    "# opt.minimize(y, 1.0, method = 'BFGS', tol = 1e-8, options = {'disp' : True, 'maxiter' : 1000})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding global optima\n",
    "\n",
    "As a final example, we want to talk about finding global optima in a function that may have many local optima.  Generally speaking, the higher dimensional your optimization problem becomes (i.e., the more parameters you are trying to fit), the more likely you are to have many, many local minima.  But what you are actually interested in is a global minimum...and these can be very difficult to find.  As a simple example, we'll look at a univariate function of x that has many local minima but only one global minimum:\n",
    "\n",
    "$$k(x) =  \\frac{\\sin (10 \\pi x)}{2x} + \\left(x - 1\\right)^4$$\n",
    "\n",
    "Plot the function on the domain $x = [0.1, 2.5]$; see for yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = lambda x: np.sin(10*np.pi*x)/2/x + (x - 1)**4\n",
    "xplot = np.linspace(0.1, 2.5, 1000)\n",
    "kplot = k(xplot)\n",
    "plt.plot(xplot, kplot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, there are many local minima on this domain, but only one global minimum somewhere around $x = 0.53$.  How to find it exactly?  Try out some of our above optimization routines with varied initial guesses.  \n",
    "\n",
    "For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xguess = 0.25\n",
    "sol = opt.minimize(k,xguess, bounds = [(0.1, 2.5)])\n",
    "plt.figure(1, figsize = (5, 5))\n",
    "plt.plot(xplot, kplot, color = 'black', linewidth = 1, label = 'k(x)')\n",
    "plt.scatter(sol.x[0], sol.fun, color = 'red', marker = 'o', label = 'minimum that we found')\n",
    "plt.legend()\n",
    "\n",
    "print(f'The for an initial guess of {xguess}, the minimum we find is a value of k = {k(sol.x[0]):3.3f} at x = {sol.x[0]:3.3f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You may want to try a global optimization algorithm...\n",
    "\n",
    "This is the type of case where we might be interested in trying out a global optimization routine, of which there are several available in Scipy: `basinhopping`, `brute`, `differential_evolution`, `shgo`, and `dual_annealing`.  They all have fairly similar syntax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xguess = 1\n",
    "sol   = opt.basinhopping(k, 1)\n",
    "plt.figure(1, figsize = (5, 5))\n",
    "plt.plot(xplot, kplot, color = 'black', linewidth = 1, label = 'k(x)')\n",
    "plt.scatter(sol.x[0], sol.fun, color = 'red', marker = 'o', label = 'minimum that we found')\n",
    "plt.legend()\n",
    "\n",
    "print(f'The for an initial guess of {xguess}, the minimum we find is a value of k = {k(sol.x[0]):3.3f} at x = {sol.x[0]:3.3f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xplot = np.linspace(0.1, 2.5, 100)\n",
    "kplot = k(xplot)\n",
    "sol   = opt.dual_annealing(k, [(0.1, 2.5)])\n",
    "plt.figure(1, figsize = (5, 5))\n",
    "plt.plot(xplot, kplot, color = 'black', linewidth = 1, label = 'k(x)')\n",
    "plt.scatter(sol.x[0], sol.fun, color = 'red', marker = 'o', label = 'minimum that we found')\n",
    "plt.legend()\n",
    "\n",
    "print(f'The minimum we find is a value of k = {k(sol.x[0]):3.3f} at x = {sol.x[0]:3.3f}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
